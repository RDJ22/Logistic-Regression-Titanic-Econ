{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Solution\n",
    "\n",
    "#### 1. Derive the Gradient Vector of the Log-Likelihood Function with Respect to $\\beta$\n",
    "\n",
    "##### The logistic regression probability model is given by:\n",
    "$$\n",
    "P(y = 1 | X) = \\frac{1}{1 + \\exp(-X \\beta)}\n",
    "$$\n",
    "##### where $X$ is the $n \\times p$ design matrix and $\\beta$ is the $p \\times 1$ vector of coefficients.\n",
    "\n",
    "##### The log-likelihood function for logistic regression is:\n",
    "$$\n",
    "\\log \\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log \\left( \\frac{1}{1 + \\exp(-X_i \\beta)} \\right) + (1 - y_i) \\log \\left( 1 - \\frac{1}{1 + \\exp(-X_i \\beta)} \\right) \\right]\n",
    "$$\n",
    "##### where $X_i$ is the $i$-th row of $X$, and $y_i$ is the $i$-th observed response.\n",
    "\n",
    "##### To simplify this, rewrite the probability:\n",
    "$$\n",
    "P(y = 1 | X_i) = \\frac{1}{1 + \\exp(-X_i \\beta)} = \\sigma(X_i \\beta)\n",
    "$$\n",
    "##### Where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.\n",
    "\n",
    "##### Thus, the log-likelihood can be rewritten as:\n",
    "$$\n",
    "\\log \\mathcal{L}(\\beta) = \\sum_{i=1}^n \\left[ y_i \\log(\\sigma(X_i \\beta)) + (1 - y_i) \\log(1 - \\sigma(X_i \\beta)) \\right]\n",
    "$$\n",
    "\n",
    "##### Let’s take the derivative of $\\log \\mathcal{L}(\\beta)$ with respect to $\\beta$:\n",
    "\n",
    "#### Step 1: Compute $\\frac{\\partial}{\\partial \\beta} \\log \\sigma(X_i \\beta)$ and $\\frac{\\partial}{\\partial \\beta} \\log(1 - \\sigma(X_i \\beta))$\n",
    "\n",
    "##### i. The derivative of $\\sigma(X_i \\beta)$ with respect to $\\beta$ is:\n",
    "   $$\n",
    "   \\frac{\\partial \\sigma(X_i \\beta)}{\\partial \\beta} = \\sigma(X_i \\beta)(1 - \\sigma(X_i \\beta)) X_i\n",
    "   $$\n",
    "\n",
    "##### ii. Using this, the derivative of $\\log \\sigma(X_i \\beta)$ with respect to $\\beta$ is:\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial \\beta} \\log(\\sigma(X_i \\beta)) = (1 - \\sigma(X_i \\beta)) X_i\n",
    "   $$\n",
    "\n",
    "##### iii. Similarly, the derivative of $\\log(1 - \\sigma(X_i \\beta))$ with respect to $\\beta$ is:\n",
    "   $$\n",
    "   \\frac{\\partial}{\\partial \\beta} \\log(1 - \\sigma(X_i \\beta)) = -\\sigma(X_i \\beta) X_i\n",
    "   $$\n",
    "\n",
    "#### Step 2: Combine Terms\n",
    "\n",
    "##### The derivative of the log-likelihood function with respect to $\\beta$ is:\n",
    "$$\n",
    "\\frac{\\partial \\log \\mathcal{L}(\\beta)}{\\partial \\beta} = \\sum_{i=1}^n \\left[ y_i (1 - \\sigma(X_i \\beta)) X_i - (1 - y_i) \\sigma(X_i \\beta) X_i \\right]\n",
    "$$\n",
    "\n",
    "##### Simplify by factoring out $X_i$:\n",
    "$$\n",
    "= \\sum_{i=1}^n \\left[ y_i - \\sigma(X_i \\beta) \\right] X_i\n",
    "$$\n",
    "\n",
    "##### Therefore, the gradient vector of the log-likelihood function with respect to $\\beta$ is:\n",
    "$$\n",
    "\\frac{\\partial \\log \\mathcal{L}(\\beta)}{\\partial \\beta} = \\sum_{i=1}^n \\left( y_i - \\sigma(X_i \\beta) \\right) X_i\n",
    "$$\n",
    "\n",
    "##### Or, in matrix form:\n",
    "$$\n",
    "\\frac{\\partial \\log \\mathcal{L}(\\beta)}{\\partial \\beta} = X^T (y - \\sigma(X \\beta))\n",
    "$$\n",
    "##### where $\\sigma(X \\beta)$ is the vector of predicted probabilities for all observations, and $y$ is the vector of observed outcomes.\n",
    "\n",
    "#### 2. Update Rule Using Gradient Descent\n",
    "\n",
    "##### To maximize the log-likelihood function, we use gradient ascent (or equivalently, to minimize the negative log-likelihood, we use gradient descent).\n",
    "\n",
    "##### The gradient ascent update rule for $\\beta$ is:\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} + \\alpha \\nabla \\log \\mathcal{L}(\\beta)\n",
    "$$\n",
    "##### where $\\alpha$ is the learning rate, and $\\nabla \\log \\mathcal{L}(\\beta)$ is the gradient of the log-likelihood function with respect to $\\beta$.\n",
    "\n",
    "##### Using the gradient derived in the previous section, the update rule becomes:\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} + \\alpha X^T (y - \\sigma(X \\beta^{(t)}))\n",
    "$$\n",
    "\n",
    "#### Step-by-Step Update Process\n",
    "\n",
    "##### i. Initialize $\\beta$ (often with zeros or small random values).\n",
    "##### ii. Compute the Predicted Probabilities: For each observation, calculate $\\sigma(X_i \\beta^{(t)})$.\n",
    "##### iii. Compute the Gradient: Use $\\nabla \\log \\mathcal{L}(\\beta) = X^T (y - \\sigma(X \\beta^{(t)}))$.\n",
    "##### iv. Update $\\beta$: Update $\\beta$ using the rule $\\beta^{(t+1)} = \\beta^{(t)} + \\alpha X^T (y - \\sigma(X \\beta^{(t)}))$.\n",
    "##### v. Repeat steps 2–4 until convergence, which is achieved when changes in $\\beta$ are below a chosen tolerance level or after a set number of iterations. \n",
    "\n",
    "##### This iterative process refines the values of $\\beta$, allowing the model to better estimate the probability $P(y = 1 | X)$ through logistic regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "##### Hastie, T., Friedman, J., & Tibshirani, R. (2001). The elements of statistical learning. In Springer series in statistics.\n",
    "##### Menard, S.W. (2010). Logistic regression: From introductory to advanced concepts and applications. Sage."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
